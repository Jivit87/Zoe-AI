# ðŸŽ™ï¸ Integrating Baileys WhatsApp with Sara Voice AI Agent

**Complete Step-by-Step Guide**

This guide will help you integrate WhatsApp messaging (via Baileys) with your existing Sara voice AI agent, enabling text message interactions.

> **Note:** This guide assumes you already have the Sara AI project set up. Sara is currently at Week 1 MVP status with full voice-to-voice conversation capabilities. Refer to `info.md` for complete project details.

---

## ðŸ“‹ Table of Contents

1. [Prerequisites](#prerequisites)
2. [Current Project Structure](#current-project-structure)
3. [Part 1: WhatsApp Bridge Setup (Node.js)](#part-1-whatsapp-bridge-setup)
4. [Part 2: Integrate with Existing Sara](#part-2-integrate-with-existing-sara)
5. [Part 3: Testing](#part-3-testing)
6. [Troubleshooting](#troubleshooting)
7. [FAQ](#faq)

---

## Prerequisites

### Required Software

- [ ] **Node.js** v18+ and npm ([Download](https://nodejs.org/))
- [ ] **Python** 3.9+ ([Download](https://python.org/))
- [ ] **Git** ([Download](https://git-scm.com/))

### API Keys Required

- [ ] **Groq API Key** (Already configured - used by Sara's brain)
- [ ] **WhatsApp Account** (For scanning QR code with Baileys)

> **Note:** Ollama is listed in requirements but not actively used in Week 1 MVP. Memory system uses lightweight JSON/Markdown storage for speed.

### Check Installation

```bash
# Verify installations
node --version    # Should be v18+
python --version  # Should be 3.9+
```

---

## Current Project Structure

Your existing Sara AI project (Week 1 MVP) has this structure:

```
sara-ai-project/                  # Your existing project
â”‚
â”œâ”€â”€ src/                          # Existing Sara components
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ sara_brain.py        # âœ… Already exists - Groq-powered AI with enhanced personality
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â””â”€â”€ conversation_memory.py # âœ… Already exists - Lightweight memory system
â”‚   â”œâ”€â”€ stt/
â”‚   â”‚   â”œâ”€â”€ speech_recognizer.py  # âœ… Already exists - Faster-Whisper STT
â”‚   â”‚   â””â”€â”€ voice_activity_detector.py # âœ… Already exists - Silero-VAD
â”‚   â”œâ”€â”€ tts/
â”‚   â”‚   â””â”€â”€ voice_generator.py    # âœ… Already exists - Kokoro-82M ONNX TTS
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ kokoro/              # âœ… Already exists - TTS model files
â”‚   â”œâ”€â”€ connectors/               # ðŸ†• We'll add WhatsApp here
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ whatsapp_client.py    # ðŸ†• To be created
â”‚   â”‚   â””â”€â”€ whatsapp_handler.py   # ðŸ†• To be created
â”‚   â””â”€â”€ main.py                   # âœ… Already exists - Main voice loop
â”‚
â”œâ”€â”€ whatsapp-bridge/              # ðŸ†• New Node.js service (directory exists but empty)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ baileys.service.js
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.js
â”‚   â”‚   â”‚   â””â”€â”€ websocket.js
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”œâ”€â”€ auth/                     # Auto-generated by Baileys (exists but for future use)
â”‚   â”œâ”€â”€ media/                    # Temp media storage (exists but empty)
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ conversations/                # âœ… Already exists - Markdown conversation logs
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chroma_db/               # âœ… Already exists - Vector DB (initialized but minimal usage)
â”œâ”€â”€ requirements.txt              # âœ… Already exists - Week 1 dependencies
â”œâ”€â”€ .env                          # âœ… Already exists - API keys
â”œâ”€â”€ info.md                       # âœ… Complete project documentation
â””â”€â”€ test_*.py                     # âœ… Test files for each component
```

**Current Status:**
- âœ… Voice-to-voice conversation loop fully functional
- âœ… Sara's personality and emotional intelligence implemented
- âœ… Memory system with markdown logging
- âœ… Proactive engagement and silence monitoring
- ðŸ†• WhatsApp bridge directory exists but not implemented

**What we'll add:**
- `whatsapp-bridge/` - New Node.js service for WhatsApp (implement from scratch)
- `src/connectors/` - WhatsApp integration code
- Update `requirements.txt` with WhatsApp dependencies

---

## Part 1: WhatsApp Bridge Setup

### Step 1.1: Create WhatsApp Bridge Directory

```bash
# Navigate to your existing Sara project root
cd /path/to/your/sara-project

# Create WhatsApp bridge directory
mkdir whatsapp-bridge
cd whatsapp-bridge
```

### Step 1.2: Initialize Node.js Project

```bash
npm init -y
```

### Step 1.3: Install Dependencies

```bash
npm install @whiskeysockets/baileys@latest express cors socket.io qrcode-terminal pino dotenv
npm install --save-dev nodemon
```

### Step 1.4: Create Package.json Scripts

Edit `package.json` and add:

```json
{
  "name": "sara-whatsapp-bridge",
  "version": "1.0.0",
  "type": "module",
  "scripts": {
    "start": "node src/index.js",
    "dev": "nodemon src/index.js"
  },
  "dependencies": {
    "@whiskeysockets/baileys": "^6.7.8",
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "socket.io": "^4.6.1",
    "qrcode-terminal": "^0.12.0",
    "pino": "^8.19.0",
    "dotenv": "^16.4.1"
  },
  "devDependencies": {
    "nodemon": "^3.0.1"
  }
}
```

### Step 1.5: Create Directory Structure

```bash
mkdir -p src/services src/api auth media
```

### Step 1.6: Create `.env` File

Create `whatsapp-bridge/.env`:

```env
PORT=3001
PYTHON_SERVICE_URL=http://localhost:8000
LOG_LEVEL=info
MEDIA_DIR=./media
```

### Step 1.7: Create Baileys Service

Create `whatsapp-bridge/src/services/baileys.service.js`:

```javascript
import makeWASocket, {
  useMultiFileAuthState,
  DisconnectReason,
  fetchLatestBaileysVersion,
  makeCacheableSignalKeyStore,
  makeInMemoryStore,
  downloadMediaMessage
} from '@whiskeysockets/baileys';
import qrcode from 'qrcode-terminal';
import { Boom } from '@hapi/boom';
import path from 'path';
import fs from 'fs';

export class BaileysService {
  constructor(io, logger, mediaDir) {
    this.io = io;
    this.logger = logger;
    this.mediaDir = mediaDir;
    this.sock = null;
    this.qrCode = null;
    this.store = makeInMemoryStore({});
    this.contacts = new Map();

    // Ensure media directory exists
    if (!fs.existsSync(mediaDir)) {
      fs.mkdirSync(mediaDir, { recursive: true });
    }
  }

  async initialize() {
    const { state, saveCreds } = await useMultiFileAuthState('./auth');
    const { version } = await fetchLatestBaileysVersion();

    this.sock = makeWASocket({
      version,
      logger: this.logger,
      printQRInTerminal: false,
      auth: {
        creds: state.creds,
        keys: makeCacheableSignalKeyStore(state.keys, this.logger)
      },
      getMessage: async (key) => {
        if (this.store) {
          const msg = await this.store.loadMessage(key.remoteJid, key.id);
          return msg?.message || undefined;
        }
        return undefined;
      }
    });

    this.store.bind(this.sock.ev);

    // Connection updates
    this.sock.ev.on('connection.update', async (update) => {
      await this.handleConnectionUpdate(update, saveCreds);
    });

    this.sock.ev.on('creds.update', saveCreds);

    // Incoming messages
    this.sock.ev.on('messages.upsert', async ({ messages, type }) => {
      await this.handleIncomingMessages(messages, type);
    });

    // Contacts
    this.sock.ev.on('contacts.update', (updates) => {
      this.updateContacts(updates);
    });

    this.logger.info('Baileys service initialized');
  }

  async handleConnectionUpdate(update, saveCreds) {
    const { connection, lastDisconnect, qr } = update;

    if (qr) {
      this.qrCode = qr;
      qrcode.generate(qr, { small: true });
      this.io.emit('qr', qr);
      this.logger.info('QR Code generated - scan with WhatsApp');
    }

    if (connection === 'close') {
      const shouldReconnect = 
        (lastDisconnect?.error instanceof Boom)
          ? lastDisconnect.error.output.statusCode !== DisconnectReason.loggedOut
          : true;

      this.logger.info(`Connection closed. Reconnecting: ${shouldReconnect}`);

      if (shouldReconnect) {
        await this.initialize();
      }
    } else if (connection === 'open') {
      this.logger.info('âœ… WhatsApp connection established');
      this.io.emit('connected', { status: 'connected' });
      this.qrCode = null;

      // Load contacts
      await this.loadContacts();
    }
  }

  async handleIncomingMessages(messages, type) {
    for (const msg of messages) {
      if (msg.key.fromMe || msg.key.remoteJid === 'status@broadcast') continue;

      try {
        const messageData = await this.extractMessageData(msg);
        
        // Emit to Python
        this.io.emit('message:received', messageData);
        
        this.logger.info(`ðŸ“© Message from ${messageData.pushName}: ${messageData.type}`);
      } catch (error) {
        this.logger.error(`Error processing message: ${error.message}`);
      }
    }
  }

  async extractMessageData(msg) {
    const messageType = Object.keys(msg.message || {})[0];
    const sender = msg.key.remoteJid;
    const pushName = msg.pushName || '';

    let messageData = {
      id: msg.key.id,
      from: sender,
      pushName: pushName,
      timestamp: msg.messageTimestamp,
      type: messageType,
      isGroup: sender.endsWith('@g.us')
    };

    // Handle different message types
    switch (messageType) {
      case 'conversation':
        messageData.text = msg.message.conversation;
        break;

      case 'extendedTextMessage':
        messageData.text = msg.message.extendedTextMessage.text;
        break;

      case 'imageMessage':
        messageData.text = msg.message.imageMessage.caption || '[Image]';
        messageData.media = {
          type: 'image',
          mimetype: msg.message.imageMessage.mimetype
        };
        break;

      case 'videoMessage':
        messageData.text = msg.message.videoMessage.caption || '[Video]';
        messageData.media = {
          type: 'video',
          mimetype: msg.message.videoMessage.mimetype
        };
        break;

      default:
        messageData.text = '[Unsupported message type]';
    }

    return messageData;
  }

  async sendMessage(jid, content, options = {}) {
    try {
      if (!this.sock) throw new Error('WhatsApp not connected');

      let messageContent;

      if (typeof content === 'string') {
        messageContent = { text: content };
      } else if (content.image) {
        messageContent = {
          image: content.image,
          caption: content.caption || ''
        };
      } else {
        messageContent = content;
      }

      const result = await this.sock.sendMessage(jid, messageContent, options);
      this.logger.info(`âœ… Message sent to ${jid}`);
      
      return { 
        success: true, 
        messageId: result.key.id,
        timestamp: result.messageTimestamp 
      };
    } catch (error) {
      this.logger.error(`Error sending message: ${error.message}`);
      throw error;
    }
  }

  async sendTyping(jid, isTyping = true) {
    try {
      await this.sock.sendPresenceUpdate(
        isTyping ? 'composing' : 'paused',
        jid
      );
      return { success: true };
    } catch (error) {
      this.logger.error(`Error sending typing: ${error.message}`);
      throw error;
    }
  }

  async sendReadReceipt(messageKey) {
    try {
      await this.sock.readMessages([messageKey]);
      return { success: true };
    } catch (error) {
      this.logger.error(`Error sending read receipt: ${error.message}`);
      throw error;
    }
  }

  async loadContacts() {
    try {
      const contacts = this.sock.store?.contacts || {};
      
      for (const [jid, contact] of Object.entries(contacts)) {
        if (jid.endsWith('@s.whatsapp.net')) {
          this.contacts.set(jid, {
            jid: jid,
            name: contact.name || contact.notify || '',
            phone: jid.replace('@s.whatsapp.net', '')
          });
        }
      }
      
      this.logger.info(`Loaded ${this.contacts.size} contacts`);
      this.io.emit('contacts:loaded', {
        contacts: Array.from(this.contacts.values())
      });
    } catch (error) {
      this.logger.error(`Error loading contacts: ${error.message}`);
    }
  }

  updateContacts(updates) {
    for (const update of updates) {
      if (update.id?.endsWith('@s.whatsapp.net')) {
        this.contacts.set(update.id, {
          jid: update.id,
          name: update.name || update.notify || '',
          phone: update.id.replace('@s.whatsapp.net', '')
        });
      }
    }
  }

  searchContact(query) {
    const queryLower = query.toLowerCase().trim();
    const results = [];

    for (const contact of this.contacts.values()) {
      const name = contact.name.toLowerCase();
      
      if (name === queryLower) {
        return [contact];
      }
      
      if (name.includes(queryLower)) {
        results.push({
          ...contact,
          score: name.startsWith(queryLower) ? 2 : 1
        });
      }
    }

    results.sort((a, b) => b.score - a.score);
    return results;
  }

  getAllContacts() {
    return Array.from(this.contacts.values());
  }

  isConnected() {
    return this.sock && this.sock.user;
  }

  getQRCode() {
    return this.qrCode;
  }

  async disconnect() {
    if (this.sock) {
      await this.sock.logout();
      this.logger.info('Disconnected from WhatsApp');
    }
  }
}
```

### Step 1.8: Create API Routes

Create `whatsapp-bridge/src/api/routes.js`:

```javascript
export function setupRoutes(app, baileysService) {
  // Health check
  app.get('/health', (req, res) => {
    res.json({
      status: 'ok',
      connected: baileysService.isConnected(),
      timestamp: new Date().toISOString()
    });
  });

  // Get QR code
  app.get('/qr', (req, res) => {
    const qr = baileysService.getQRCode();
    if (qr) {
      res.json({ qr });
    } else {
      res.status(404).json({ error: 'No QR code available' });
    }
  });

  // Connection status
  app.get('/status', (req, res) => {
    res.json({
      connected: baileysService.isConnected(),
      user: baileysService.sock?.user || null
    });
  });

  // Send text message
  app.post('/send', async (req, res) => {
    try {
      const { jid, message } = req.body;

      if (!jid || !message) {
        return res.status(400).json({ error: 'Missing required fields' });
      }

      const result = await baileysService.sendMessage(jid, message);
      res.json(result);
    } catch (error) {
      res.status(500).json({ error: error.message });
    }
  });

  // Send typing indicator
  app.post('/typing/:jid', async (req, res) => {
    try {
      const { jid } = req.params;
      const { isTyping = true } = req.body;
      const result = await baileysService.sendTyping(jid, isTyping);
      res.json(result);
    } catch (error) {
      res.status(500).json({ error: error.message });
    }
  });

  // Get all contacts
  app.get('/contacts', (req, res) => {
    const contacts = baileysService.getAllContacts();
    res.json({ contacts, count: contacts.length });
  });

  // Search contacts
  app.get('/contacts/search', (req, res) => {
    const { query } = req.query;
    
    if (!query) {
      return res.status(400).json({ error: 'Query parameter required' });
    }

    const results = baileysService.searchContact(query);
    res.json({ results });
  });
}
```

### Step 1.9: Create WebSocket Handler

Create `whatsapp-bridge/src/api/websocket.js`:

```javascript
export function setupWebSocket(io, baileysService) {
  io.on('connection', (socket) => {
    console.log('âœ… Python client connected');

    // Send current status
    socket.emit('status', {
      connected: baileysService.isConnected()
    });

    socket.on('disconnect', () => {
      console.log('âŒ Python client disconnected');
    });

    // Handle send message from Python
    socket.on('send:message', async (data) => {
      try {
        await baileysService.sendMessage(data.jid, data.message);
        socket.emit('send:success', { messageId: data.id });
      } catch (error) {
        socket.emit('send:error', { error: error.message });
      }
    });
  });
}
```

### Step 1.10: Create Main Entry Point

Create `whatsapp-bridge/src/index.js`:

```javascript
import express from 'express';
import cors from 'cors';
import { Server } from 'socket.io';
import { createServer } from 'http';
import dotenv from 'dotenv';
import pino from 'pino';
import { BaileysService } from './services/baileys.service.js';
import { setupRoutes } from './api/routes.js';
import { setupWebSocket } from './api/websocket.js';

dotenv.config();

const logger = pino({ level: process.env.LOG_LEVEL || 'info' });
const app = express();
const server = createServer(app);
const io = new Server(server, {
  cors: {
    origin: process.env.PYTHON_SERVICE_URL || 'http://localhost:8000',
    methods: ['GET', 'POST']
  }
});

// Middleware
app.use(cors());
app.use(express.json());

// Initialize Baileys
const mediaDir = process.env.MEDIA_DIR || './media';
const baileysService = new BaileysService(io, logger, mediaDir);

// Setup routes and WebSocket
setupRoutes(app, baileysService);
setupWebSocket(io, baileysService);

// Start server
const PORT = process.env.PORT || 3001;
server.listen(PORT, async () => {
  logger.info(`ðŸš€ WhatsApp Bridge running on port ${PORT}`);
  await baileysService.initialize();
});

// Graceful shutdown
process.on('SIGINT', async () => {
  logger.info('Shutting down...');
  await baileysService.disconnect();
  process.exit(0);
});
```

### Step 1.11: Create .gitignore

Create `whatsapp-bridge/.gitignore`:

```
node_modules/
auth/
media/
.env
*.log
```

---

## Part 2: Integrate with Existing Sara

### Step 2.1: Update Requirements

Add WhatsApp dependencies to your existing `requirements.txt`:

```bash
# Navigate back to project root
cd ..

# Add to requirements.txt
cat >> requirements.txt << 'EOF'

# WhatsApp Integration
python-socketio[asyncio]>=5.11.0
aiohttp>=3.9.1
requests>=2.31.0
EOF

# Install new dependencies
pip install python-socketio[asyncio] aiohttp requests
```

### Step 2.2: Update .env File

Add WhatsApp Bridge URL to your existing `.env`:

```bash
# Add to .env
echo "WHATSAPP_BRIDGE_URL=http://localhost:3001" >> .env
```

### Step 2.3: Create Connectors Directory

```bash
# Create connectors directory if it doesn't exist
mkdir -p src/connectors
touch src/connectors/__init__.py
```

### Step 2.4: Create WhatsApp Client

Create `src/connectors/whatsapp_client.py`:

```python
"""
WhatsApp Client for Sara AI
============================
Communicates with WhatsApp Bridge (Node.js/Baileys) via WebSocket and REST API.
Integrates with Sara's existing architecture.
"""

import socketio
import requests
import logging
from typing import Callable, Optional

logger = logging.getLogger(__name__)


class WhatsAppClient:
    """Client to communicate with WhatsApp Bridge"""
    
    def __init__(self, bridge_url: str = "http://localhost:3001"):
        self.bridge_url = bridge_url
        self.sio = socketio.AsyncClient()
        self.message_handler: Optional[Callable] = None
        self.connected = False
        
        self.setup_events()
    
    def setup_events(self):
        """Setup WebSocket event handlers"""
        
        @self.sio.event
        async def connect():
            logger.info("âœ… Connected to WhatsApp Bridge")
            self.connected = True
        
        @self.sio.event
        async def disconnect():
            logger.info("âŒ Disconnected from WhatsApp Bridge")
            self.connected = False
        
        @self.sio.event
        async def qr(data):
            logger.info("\n" + "="*60)
            logger.info("ðŸ“± SCAN THIS QR CODE WITH WHATSAPP")
            logger.info("="*60)
            logger.info("Open WhatsApp â†’ Settings â†’ Linked Devices â†’ Link a Device")
            logger.info("="*60 + "\n")
        
        @self.sio.on('message:received')
        async def on_message(data):
            """Handle incoming WhatsApp messages"""
            if self.message_handler:
                await self.message_handler(data)
        
        @self.sio.on('connected')
        async def on_connected(data):
            logger.info("âœ… WhatsApp connected successfully!")
    
    async def connect(self):
        """Connect to WhatsApp Bridge"""
        try:
            await self.sio.connect(self.bridge_url)
            logger.info("Initializing WhatsApp client...")
        except Exception as e:
            logger.error(f"Failed to connect: {e}")
            raise
    
    async def disconnect(self):
        """Disconnect from WhatsApp Bridge"""
        await self.sio.disconnect()
    
    def set_message_handler(self, handler: Callable):
        """Set callback for incoming messages"""
        self.message_handler = handler
    
    async def send_text(self, jid: str, text: str) -> dict:
        """Send text message"""
        try:
            response = requests.post(
                f"{self.bridge_url}/send",
                json={
                    "jid": jid,
                    "message": text
                },
                timeout=10
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Failed to send text: {e}")
            raise
    
    def send_typing(self, jid: str, is_typing: bool = True) -> dict:
        """Send typing indicator"""
        try:
            response = requests.post(
                f"{self.bridge_url}/typing/{jid}",
                json={"isTyping": is_typing},
                timeout=5
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Failed to send typing: {e}")
            return {}
    
    def is_ready(self) -> bool:
        """Check if WhatsApp is connected"""
        try:
            response = requests.get(f"{self.bridge_url}/status", timeout=5)
            data = response.json()
            return data.get("connected", False)
        except:
            return False
```

### Step 2.5: Create WhatsApp Message Handler

Create `src/connectors/whatsapp_handler.py`:

```python
"""
WhatsApp Message Handler for Sara AI
=====================================
Integrates Sara's existing AI brain with WhatsApp messaging.
Uses Sara's enhanced personality and emotional intelligence.
"""

import logging
import asyncio
from pathlib import Path
from typing import Dict

# Import existing Sara components
from src.llm.sara_brain import SaraBrain
from src.connectors.whatsapp_client import WhatsAppClient

logger = logging.getLogger(__name__)


class WhatsAppHandler:
    """Handle WhatsApp messages using Sara's existing brain"""
    
    def __init__(self, whatsapp_client: WhatsAppClient, sara_brain: SaraBrain):
        self.whatsapp = whatsapp_client
        self.sara = sara_brain
        
        # Set message handler
        self.whatsapp.set_message_handler(self.handle_message)
    
    async def handle_message(self, message_data: Dict):
        """
        Handle incoming WhatsApp message
        
        Flow:
        1. Receive text message
        2. Process with Sara's existing AI brain (Groq + Enhanced Personality)
        3. Send text response
        """
        try:
            from_jid = message_data['from']
            user_name = message_data['pushName']
            message_type = message_data['type']
            is_group = message_data.get('isGroup', False)
            
            # Skip group messages
            if is_group:
                logger.info("â­ï¸  Skipping group message")
                return
            
            logger.info("\n" + "="*60)
            logger.info(f"ðŸ“© Message from {user_name}")
            logger.info(f"   Type: {message_type}")
            
            # Show typing indicator
            self.whatsapp.send_typing(from_jid, True)
            
            # Extract text from message
            text = message_data.get('text', '')
            logger.info(f"   ðŸ’¬ Text: {text}")
            
            if not text:
                logger.warning("No text in message")
                return
            
            # Process with Sara's existing brain
            # Uses Sara's enhanced personality, emotional intelligence, and memory
            sara_response = self.sara.generate_response(
                user_input=text,
                emotional_state="neutral",
                visual_context=f"WhatsApp message from {user_name}"
            )
            
            logger.info(f"   ðŸ¤– Sara: {sara_response}")
            
            # Send text response
            logger.info(f"   ðŸ’¬ Sending text response...")
            await self.whatsapp.send_text(from_jid, sara_response)
            
            # Stop typing
            self.whatsapp.send_typing(from_jid, False)
            
            logger.info("   âœ… Response sent!")
            logger.info("="*60 + "\n")
        
        except Exception as e:
            logger.error(f"âŒ Error handling message: {e}", exc_info=True)
            
            # Send error message
            try:
                await self.whatsapp.send_text(
                    message_data['from'],
                    "I apologize, I encountered an error processing your message."
                )
            except:
                pass
```

### Step 2.6: Create WhatsApp Main Application

Create `src/whatsapp_main.py`:

```python
"""
Sara AI - WhatsApp Integration
===============================
Main entry point for WhatsApp messaging.
Integrates with Sara's existing Week 1 MVP components.
"""

import asyncio
import logging
import os
from pathlib import Path
from dotenv import load_dotenv

# Import existing Sara components
from src.llm.sara_brain import SaraBrain
from src.connectors.whatsapp_client import WhatsAppClient
from src.connectors.whatsapp_handler import WhatsAppHandler

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SaraWhatsAppAgent:
    """Sara AI integrated with WhatsApp - uses existing Week 1 MVP components"""
    
    def __init__(self):
        # Get WhatsApp Bridge URL from environment
        bridge_url = os.getenv("WHATSAPP_BRIDGE_URL", "http://localhost:3001")
        
        # Initialize components (using existing Sara brain from Week 1 MVP)
        logger.info("ðŸ§  Initializing Sara's brain (Groq + Enhanced Personality)...")
        self.sara = SaraBrain()
        
        logger.info("ðŸ“± Initializing WhatsApp client...")
        self.whatsapp = WhatsAppClient(bridge_url)
        
        logger.info("ðŸ”— Connecting handler...")
        self.handler = WhatsAppHandler(
            whatsapp_client=self.whatsapp,
            sara_brain=self.sara
        )
    
    async def initialize(self):
        """Initialize connections"""
        logger.info("ðŸš€ Starting Sara WhatsApp Agent...")
        
        # Connect to WhatsApp Bridge
        await self.whatsapp.connect()
        
        # Wait for WhatsApp to be ready
        logger.info("â³ Waiting for WhatsApp connection...")
        while not self.whatsapp.is_ready():
            await asyncio.sleep(5)
        
        logger.info("âœ… WhatsApp connected and ready!")
    
    async def run(self):
        """Main run loop"""
        await self.initialize()
        
        logger.info("\n" + "="*60)
        logger.info("ðŸŽ™ï¸  SARA AI - WHATSAPP AGENT")
        logger.info("="*60)
        logger.info("\nFeatures:")
        logger.info("  âœ“ Receives text messages")
        logger.info("  âœ“ Processes with Sara AI (Groq llama-3.3-70b-versatile)")
        logger.info("  âœ“ Enhanced personality with emotional intelligence")
        logger.info("  âœ“ Responds with text messages")
        logger.info("  âœ“ Remembers all conversations (markdown logs)")
        logger.info("  âœ“ Natural conversation patterns")
        logger.info("\nðŸ“± Send a WhatsApp message to your number to start!")
        logger.info("="*60 + "\n")
        
        # Keep running
        try:
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            logger.info("\nðŸ‘‹ Shutting down Sara...")
            
            # Save conversation history
            self.sara.memory.save_session_to_markdown()
            logger.info("âœ“ Conversation saved to conversations/")
            
            await self.whatsapp.disconnect()


if __name__ == "__main__":
    app = SaraWhatsAppAgent()
    asyncio.run(app.run())
```

---

## Part 3: Testing

### Step 3.1: Start WhatsApp Bridge

```bash
# Terminal 1
cd whatsapp-bridge
npm run dev

# You should see:
# ðŸš€ WhatsApp Bridge running on port 3001
# QR Code will appear in terminal
```

### Step 3.2: Scan QR Code

1. Open WhatsApp on your phone
2. Go to **Settings** â†’ **Linked Devices**
3. Tap **Link a Device**
4. Scan the QR code in the terminal

### Step 3.3: Start Sara WhatsApp Agent

```bash
# Terminal 2 - In your Sara project root
python -m src.whatsapp_main

# You should see:
# ðŸŽ™ï¸ SARA AI - WHATSAPP AGENT
# âœ… WhatsApp connected and ready!
```

### Step 3.4: Test Text Message

Send a text message to your WhatsApp number:

```
You: "Hey Sara, how are you?"

Expected:
1. Sara shows typing indicator
2. Sara processes your message with AI (using existing enhanced personality)
3. Sara responds with a text message using her natural conversation style
4. Console shows the conversation:
   ðŸ“© Message from [Your Name]
      Type: conversation
      ðŸ’¬ Text: Hey Sara, how are you?
      ðŸ¤– Sara: Hey. I'm good, thanks. How's your day going?
      âœ… Response sent!
```

**Note:** Sara will respond with her characteristic personality:
- Natural speech patterns (contractions, fillers)
- Short, powerful responses (1-3 sentences)
- Emotional intelligence and context awareness
- Remembers conversation history

---

## Troubleshooting

### Issue: QR Code Not Showing

**Solution:**
```bash
# Check if bridge is running
curl http://localhost:3001/health

# Should return: {"status": "ok", ...}
```

### Issue: "WhatsApp not connected" Error

**Solution:**
1. Make sure you scanned the QR code
2. Check WhatsApp Bridge logs
3. Restart the bridge: `npm run dev`

### Issue: "Module not found" in Python

**Solution:**
```bash
# Ensure virtual environment is activated
source venv/bin/activate

# Reinstall dependencies
pip install -r requirements.txt
```

### Issue: Groq API Errors

**Solution:**
```bash
# Verify API key
echo $GROQ_API_KEY

# Test API
curl -X POST "https://api.groq.com/openai/v1/chat/completions" \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model": "llama-3.3-70b-versatile", "messages": [{"role": "user", "content": "Hello"}]}'
```

### Issue: Sara Responds Too Slowly

**Solutions:**
1. Reduce max_tokens in Sara brain
2. Use faster Groq model
3. Optimize network connection

### Issue: Connection Keeps Dropping

**Solution:**
```bash
# Add keepalive to WhatsApp Bridge
# In baileys.service.js, add:
this.sock = makeWASocket({
  // ... existing config ...
  keepAliveIntervalMs: 30000
});
```

---

## FAQ

### Q: Can I use this in production?

**A:** Baileys is unofficial and may violate WhatsApp's Terms of Service. Use at your own risk. Not recommended for commercial use.

### Q: How much does this cost?

**A:** 
- Groq API: ~$0.10 per 1M tokens (very cheap)
- Server: $5-10/month for VPS
- WhatsApp: Free

### Q: Can Sara handle multiple users?

**A:** Yes! Each conversation is tracked separately with user_id (WhatsApp JID).

### Q: Can Sara handle voice messages?

**A:** Currently, Sara only handles text messages and responds with text. Voice message support is planned for future weeks but not implemented in Week 1 MVP. The existing voice capabilities (STT/TTS) are for local microphone/speaker interaction, not WhatsApp voice notes.

### Q: How do I add multiple languages?

**A:** 
1. Update Sara's system prompt to support multiple languages
2. The AI will automatically detect and respond in the user's language

### Q: Can Sara call people?

**A:** No, Baileys doesn't support WhatsApp calls.

---

## Next Steps

1. âœ… **Basic Setup Complete** - Sara can now chat via WhatsApp!

2. **Enhance Sara (Future Weeks):**
   - Add vision capabilities (camera integration)
   - Implement emotion detection from voice
   - Add user preferences and personalization
   - Implement scheduling for proactive messages
   - Add rich media support (images, documents)
   - Voice message support for WhatsApp

3. **Production Readiness:**
   - Add error recovery
   - Implement rate limiting
   - Setup monitoring and logs
   - Deploy to VPS/cloud server

4. **Integration with Voice Loop:**
   - Combine WhatsApp messaging with local voice assistant
   - Multi-modal interaction (voice + text)
   - Unified conversation history across channels

---

**ðŸŽ‰ Congratulations!** You've successfully integrated WhatsApp with Sara AI. Sara now has two modes of interaction:
- **Voice Mode**: Local microphone/speaker conversation (Week 1 MVP)
- **Text Mode**: WhatsApp messaging (this guide)

Both modes use Sara's same enhanced personality and emotional intelligence!* You now have Sara AI integrated with WhatsApp, responding to messages with text!

Send a message to your number and watch Sara respond! ðŸŽ™ï¸

---

# ðŸŽ¤ Local Voice Assistant for WhatsApp Control

**Sara Voice Agent - Control WhatsApp with Your Voice**

> **âš ï¸ IMPORTANT NOTE:** This section describes an advanced integration that is NOT part of the Week 1 MVP. The Week 1 MVP includes:
> - âœ… Local voice-to-voice conversation (microphone â†’ Sara â†’ speaker)
> - âœ… WhatsApp text messaging (this guide above)
> 
> The voice assistant for WhatsApp control described below would require additional development to combine both capabilities. Consider this a roadmap for future enhancement.

Talk to Sara through your computer's microphone to control WhatsApp, just like talking to Alexa or Siri.

> **Note:** This guide would integrate Sara's existing voice components (`src/stt/speech_recognizer.py`, `src/tts/voice_generator.py`) with the WhatsApp bridge to enable voice-controlled WhatsApp messaging.

---

## ðŸŽ¯ What You Can Do

```
You (speaking to computer): "Hey Sara, send a message to Pranav saying I'll be late"
Sara (computer speaker): "Message sent to Pranav âœ“"

You: "Do I have any new WhatsApp messages?"
Sara: "Yes, you have 3 new messages. One from Mom saying..."

You: "Reply to Mom saying I'll be home by 8"
Sara: "Replied to Mom âœ“"
```

> **Note:** Sara responds to WhatsApp contacts with text messages (not voice notes) by default.

---

## ðŸ—ï¸ Architecture

```
Your Voice â†’ Microphone â†’ VAD â†’ Whisper STT â†’ Groq AI â†’ WhatsApp Bridge â†’ WhatsApp
              â†“
         Computer Speaker â† TTS â† Sara Response
```

**Two Modes:**
1. **Local Voice Commands** (Your computer mic) â†’ Controls WhatsApp
2. **WhatsApp Messages** (From contacts) â†’ Sara responds on WhatsApp

---

## ðŸ“ Complete Project Structure

```
sara-ai-project/                  # Your existing Sara project
â”‚
â”œâ”€â”€ whatsapp-bridge/              # Node.js (WhatsApp connection)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ baileys.service.js
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ routes.js
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”œâ”€â”€ auth/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ src/                          # Python (Voice Assistant)
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ sara_brain.py        # âœ… Already exists - AI with function calling
â”‚   â”‚
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â””â”€â”€ conversation_memory.py # âœ… Already exists - Memory system
â”‚   â”‚
â”‚   â”œâ”€â”€ stt/
â”‚   â”‚   â”œâ”€â”€ speech_recognizer.py  # âœ… Already exists - Whisper STT
â”‚   â”‚   â””â”€â”€ voice_activity_detector.py # âœ… Already exists - VAD
â”‚   â”‚
â”‚   â”œâ”€â”€ tts/
â”‚   â”‚   â””â”€â”€ voice_generator.py    # âœ… Already exists - TTS
â”‚   â”‚
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ whatsapp_client.py   # ðŸ†• WhatsApp API client
â”‚   â”‚   â”œâ”€â”€ contact_manager.py   # ðŸ†• Contact search
â”‚   â”‚   â”œâ”€â”€ whatsapp_crud.py     # ðŸ†• CRUD operations
â”‚   â”‚   â”œâ”€â”€ voice_input.py       # ðŸ†• Microphone + VAD wrapper
â”‚   â”‚   â”œâ”€â”€ voice_output.py      # ðŸ†• TTS Speaker wrapper
â”‚   â”‚   â””â”€â”€ message_queue.py     # ðŸ†• Track WhatsApp messages
â”‚   â”‚
â”‚   â””â”€â”€ main.py                   # âœ… Already exists - Main voice loop
â”‚
â”œâ”€â”€ requirements.txt              # âœ… Already exists
â”œâ”€â”€ .env                          # âœ… Already exists
â””â”€â”€ README.md
```

---

## Part 1: Local Voice Input Setup

### Step 1.1: Create Voice Input Handler

Create `src/connectors/voice_input.py`:

> **Note:** This wraps your existing `src/stt/speech_recognizer.py` and `src/stt/voice_activity_detector.py` for microphone input.

```python
import logging
import numpy as np
import sounddevice as sd
from faster_whisper import WhisperModel
import torch
from collections import deque
import time

logger = logging.getLogger(__name__)


class VoiceInput:
    """
    Capture voice from microphone with Voice Activity Detection
    
    Features:
    - Listens to microphone continuously
    - Detects when you start/stop speaking (VAD)
    - Transcribes speech to text
    """
    
    def __init__(
        self,
        sample_rate=16000,
        model_size="base",
        silence_threshold=0.02,
        silence_duration=1.5
    ):
        self.sample_rate = sample_rate
        self.silence_threshold = silence_threshold
        self.silence_duration = silence_duration
        
        # Whisper model for transcription
        self.whisper = WhisperModel(model_size, device="cpu", compute_type="int8")
        
        # Load Silero VAD
        self.vad_model, _ = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False
        )
        
        # Audio buffer
        self.audio_buffer = deque(maxlen=int(sample_rate * 10))  # 10 seconds max
        self.is_speaking = False
        self.silence_start = None
        
        logger.info(f"Voice input initialized (Whisper: {model_size})")
    
    def detect_speech(self, audio_chunk):
        """
        Detect if audio chunk contains speech using Silero VAD
        
        Args:
            audio_chunk: Audio samples (numpy array)
        
        Returns:
            True if speech detected, False otherwise
        """
        # Convert to torch tensor
        audio_tensor = torch.from_numpy(audio_chunk).float()
        
        # Get speech probability
        speech_prob = self.vad_model(audio_tensor, self.sample_rate).item()
        
        return speech_prob > 0.5
    
    def listen_for_command(self, wake_word_mode=False):
        """
        Listen for voice command
        
        Args:
            wake_word_mode: If True, wait for wake word before recording
        
        Returns:
            Transcribed text or None
        """
        logger.info("ðŸŽ¤ Listening...")
        
        self.audio_buffer.clear()
        self.is_speaking = False
        self.silence_start = None
        
        recording_audio = []
        
        def audio_callback(indata, frames, time_info, status):
            """Called for each audio chunk from microphone"""
            if status:
                logger.warning(f"Audio status: {status}")
            
            # Convert to mono if stereo
            audio = indata[:, 0] if indata.shape[1] > 1 else indata.flatten()
            
            # Detect speech
            has_speech = self.detect_speech(audio)
            
            if has_speech:
                if not self.is_speaking:
                    # Started speaking
                    self.is_speaking = True
                    logger.info("ðŸ—£ï¸  Speech detected...")
                
                recording_audio.append(audio.copy())
                self.silence_start = None
            else:
                if self.is_speaking:
                    # Was speaking, now silence
                    if self.silence_start is None:
                        self.silence_start = time.time()
                    
                    # Check if silence duration exceeded
                    if time.time() - self.silence_start > self.silence_duration:
                        # Stop recording
                        raise sd.CallbackStop()
                    
                    # Still include some silence for natural speech
                    recording_audio.append(audio.copy())
        
        try:
            # Start recording
            with sd.InputStream(
                samplerate=self.sample_rate,
                channels=1,
                callback=audio_callback,
                blocksize=int(self.sample_rate * 0.5)  # 500ms chunks
            ):
                # Wait until recording stops (silence detected)
                while True:
                    sd.sleep(100)
        
        except sd.CallbackStop:
            logger.info("ðŸ”‡ Silence detected, processing...")
        
        # Check if we got any audio
        if not recording_audio:
            logger.warning("No audio captured")
            return None
        
        # Combine all audio chunks
        full_audio = np.concatenate(recording_audio)
        
        # Transcribe
        return self.transcribe(full_audio)
    
    def transcribe(self, audio):
        """
        Transcribe audio to text using Whisper
        
        Args:
            audio: Audio samples (numpy array)
        
        Returns:
            Transcribed text
        """
        try:
            logger.info("ðŸ”„ Transcribing...")
            
            # Transcribe with Whisper
            segments, info = self.whisper.transcribe(
                audio,
                language="en",
                vad_filter=True,
                vad_parameters={
                    "threshold": 0.5,
                    "min_speech_duration_ms": 250
                }
            )
            
            # Combine all segments
            text = " ".join([segment.text for segment in segments]).strip()
            
            logger.info(f"ðŸ“ Transcription: {text}")
            return text
        
        except Exception as e:
            logger.error(f"Transcription error: {e}")
            return None
    
    def list_microphones(self):
        """List available microphones"""
        devices = sd.query_devices()
        print("\nðŸ“± Available Microphones:")
        for i, device in enumerate(devices):
            if device['max_input_channels'] > 0:
                print(f"  {i}: {device['name']}")
        print()
    
    def test_microphone(self, duration=3):
        """Test microphone by recording and playing back"""
        logger.info(f"ðŸŽ¤ Testing microphone for {duration} seconds...")
        
        recording = sd.rec(
            int(duration * self.sample_rate),
            samplerate=self.sample_rate,
            channels=1
        )
        sd.wait()
        
        logger.info("ðŸ”Š Playing back recording...")
        sd.play(recording, self.sample_rate)
        sd.wait()
        
        logger.info("âœ… Microphone test complete")


class WakeWordDetector:
    """Simple wake word detection"""
    
    def __init__(self, wake_words=["hey sara", "sara"]):
        self.wake_words = [w.lower() for w in wake_words]
    
    def detect(self, text):
        """Check if text contains wake word"""
        if not text:
            return False
        
        text_lower = text.lower()
        
        for wake_word in self.wake_words:
            if wake_word in text_lower:
                # Remove wake word from text
                cleaned = text_lower.replace(wake_word, "").strip()
                return cleaned if cleaned else True
        
        return False
```

### Step 1.2: Create Voice Output Handler

Create `src/connectors/voice_output.py`:

> **Note:** This wraps your existing `src/tts/voice_generator.py` for speaker output.

```python
import logging
import asyncio
import edge_tts
import sounddevice as sd
import soundfile as sf
from pathlib import Path
import numpy as np

logger = logging.getLogger(__name__)


class VoiceOutput:
    """
    Text-to-speech output to computer speakers
    
    Features:
    - Converts text to speech
    - Plays audio through computer speakers
    - Natural sounding voice
    """
    
    def __init__(self, voice="en-US-AriaNeural", rate="+0%", volume="+0%"):
        self.voice = voice
        self.rate = rate
        self.volume = volume
        self.temp_dir = Path("temp")
        self.temp_dir.mkdir(exist_ok=True)
        
        logger.info(f"Voice output initialized (Voice: {voice})")
    
    async def speak(self, text, play_audio=True):
        """
        Convert text to speech and play it
        
        Args:
            text: Text to speak
            play_audio: Whether to play audio (False for testing)
        
        Returns:
            Path to generated audio file
        """
        try:
            logger.info(f"ðŸ—£ï¸  Sara: {text}")
            
            # Generate unique filename
            audio_path = self.temp_dir / f"tts_{hash(text)}.mp3"
            
            # Generate speech
            communicate = edge_tts.Communicate(
                text,
                self.voice,
                rate=self.rate,
                volume=self.volume
            )
            
            await communicate.save(str(audio_path))
            
            # Play audio
            if play_audio:
                self.play_audio_file(audio_path)
            
            return audio_path
        
        except Exception as e:
            logger.error(f"TTS error: {e}")
            raise
    
    def play_audio_file(self, audio_path):
        """Play audio file through speakers"""
        try:
            # Load audio
            data, samplerate = sf.read(audio_path)
            
            # Play
            sd.play(data, samplerate)
            sd.wait()  # Wait until audio finishes
        
        except Exception as e:
            logger.error(f"Audio playback error: {e}")
    
    def speak_sync(self, text):
        """Synchronous version of speak (for non-async contexts)"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            return loop.run_until_complete(self.speak(text))
        finally:
            loop.close()
    
    @staticmethod
    def list_voices():
        """List available TTS voices"""
        async def get_voices():
            voices = await edge_tts.list_voices()
            return voices
        
        voices = asyncio.run(get_voices())
        
        print("\nðŸŽ¤ Available Voices:")
        for voice in voices[:10]:  # Show first 10
            print(f"  {voice['ShortName']}: {voice['Locale']} - {voice['Gender']}")
        print(f"\n  ... and {len(voices) - 10} more\n")
```

---

## Part 2: Main Voice Assistant

### Step 2.1: Enhanced Sara Brain

Use your existing `src/llm/sara_brain.py` with function calling support.

> **Note:** Your Sara brain already has personality and function calling. We'll use it as-is for WhatsApp CRUD operations.

### Step 2.2: Create Main Voice Assistant

Create `src/main_voice_assistant.py`:

> **Note:** This integrates with your existing Sara components and adds WhatsApp control via voice.

```python
import asyncio
import logging
from pathlib import Path
from src.connectors.voice_input import VoiceInput, WakeWordDetector
from src.connectors.voice_output import VoiceOutput
from src.llm.sara_brain import SaraBrain
from src.connectors.message_queue import MessageQueue
from src.connectors.whatsapp_client import WhatsAppClient
from src.connectors.contact_manager import ContactManager
from src.connectors.whatsapp_crud import WhatsAppCRUDHandler
import os
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SaraVoiceAssistant:
    """
    Local Voice Assistant for WhatsApp Control
    
    Features:
    - Listen to your voice via microphone
    - Execute WhatsApp commands (send, read, reply, etc.)
    - Respond via computer speakers
    - Also handles incoming WhatsApp messages
    """
    
    def __init__(self):
        # Load settings from environment
        self.whatsapp_bridge_url = os.getenv("WHATSAPP_BRIDGE_URL", "http://localhost:3001")
        self.whisper_model = os.getenv("WHISPER_MODEL", "base")
        self.tts_voice = os.getenv("TTS_VOICE", "en-US-AriaNeural")
        self.temp_dir = os.getenv("TEMP_DIR", "temp")
        
        # Voice components
        self.voice_input = VoiceInput(model_size=self.whisper_model)
        self.voice_output = VoiceOutput(voice=self.tts_voice)
        self.wake_word = WakeWordDetector(wake_words=["hey sara", "sara"])
        
        # AI & WhatsApp components (use existing Sara brain)
        self.sara = SaraBrain()
        self.whatsapp = WhatsAppClient(self.whatsapp_bridge_url)
        self.contacts = ContactManager(self.whatsapp_bridge_url)
        self.message_queue = MessageQueue()
        
        # CRUD handler
        self.crud = None  # Initialize after WhatsApp connection
        
        # State
        self.running = False
        self.voice_mode_active = True
        
        # Temp directory
        Path(self.temp_dir).mkdir(exist_ok=True)
    
    async def initialize(self):
        """Initialize all components"""
        logger.info("ðŸš€ Initializing Sara Voice Assistant...")
        
        # Test microphone
        logger.info("ðŸŽ¤ Testing microphone...")
        self.voice_input.list_microphones()
        
        # Connect to WhatsApp
        logger.info("ðŸ“± Connecting to WhatsApp...")
        await self.whatsapp.connect()
        
        # Wait for WhatsApp to be ready
        while not self.whatsapp.is_ready():
            logger.info("â³ Waiting for WhatsApp connection...")
            await asyncio.sleep(5)
        
        logger.info("âœ… WhatsApp connected!")
        
        # Load contacts
        logger.info("ðŸ“‡ Loading contacts...")
        await asyncio.sleep(2)
        self.contacts.load_contacts()
        
        # Initialize CRUD handler
        self.crud = WhatsAppCRUDHandler(
            self.whatsapp,
            self.contacts,
            self.message_queue
        )
        
        # Setup WhatsApp message handler (for incoming messages)
        self.whatsapp.set_message_handler(self.handle_incoming_whatsapp_message)
        
        logger.info("âœ… Sara Voice Assistant Ready!")
    
    async def handle_incoming_whatsapp_message(self, message_data):
        """Handle incoming WhatsApp messages (runs in background)"""
        try:
            from_jid = message_data['from']
            user_name = message_data['pushName']
            text = message_data.get('text', '')
            is_group = message_data.get('isGroup', False)
            
            # Skip groups
            if is_group:
                return
            
            logger.info(f"\nðŸ“© WhatsApp message from {user_name}: {text}")
            
            # Add to message queue
            self.message_queue.add_message(from_jid, message_data)
            
            # Notify via voice (optional)
            await self.voice_output.speak(
                f"New message from {user_name}",
                play_audio=True
            )
            
        except Exception as e:
            logger.error(f"Error handling WhatsApp message: {e}")
    
    async def execute_tool(self, function_name: str, function_args: dict) -> dict:
        """Execute tools called by Sara"""
        
        logger.info(f"ðŸ”§ Executing: {function_name}")
        
        if function_name == "send_message":
            return await self.crud.send_message(
                recipient_name=function_args['recipient_name'],
                message=function_args['message'],
                send_as_voice=function_args.get('send_as_voice', False)
            )
        
        elif function_name == "check_new_messages":
            return self.crud.check_new_messages()
        
        elif function_name == "read_messages":
            return self.crud.read_messages(
                contact_name=function_args['contact_name'],
                limit=function_args.get('limit', 5)
            )
        
        elif function_name == "reply_to_contact":
            return await self.crud.reply_to_contact(
                contact_name=function_args['contact_name'],
                message=function_args['message']
            )
        
        elif function_name == "delete_last_message":
            return self.crud.delete_last_message(
                contact_name=function_args['contact_name']
            )
        
        elif function_name == "search_contacts":
            return self.crud.search_contacts(
                query=function_args['query']
            )
        
        elif function_name == "get_conversation_summary":
            return self.crud.get_conversation_summary(
                contact_name=function_args['contact_name']
            )
        
        else:
            return {"success": False, "error": f"Unknown tool: {function_name}"}
    
    async def process_voice_command(self, command_text):
        """
        Process voice command from microphone
        
        Args:
            command_text: Transcribed text from voice input
        """
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"ðŸŽ¤ You said: {command_text}")
            
            # Process with Sara (with function calling)
            sara_response = await self.sara.process_voice_command(
                text=command_text,
                user_id="local_user",
                user_name="User",
                tool_executor=self.execute_tool,
                context={'source': 'microphone'}
            )
            
            # Speak response
            await self.voice_output.speak(sara_response)
            
            logger.info(f"{'='*60}\n")
        
        except Exception as e:
            logger.error(f"Error processing command: {e}", exc_info=True)
            await self.voice_output.speak("I'm sorry, I had trouble with that command.")
    
    async def voice_command_loop(self):
        """Main loop: Listen for wake word â†’ Process command"""
        
        logger.info("\n" + "="*60)
        logger.info("ðŸŽ¤ LISTENING FOR WAKE WORD")
        logger.info("="*60)
        logger.info("\nSay:")
        logger.info('  "Hey Sara, send a message to Pranav saying hello"')
        logger.info('  "Hey Sara, do I have any new messages?"')
        logger.info('  "Hey Sara, read messages from Mom"')
        logger.info("\nOr just:")
        logger.info('  "Sara, check my messages"')
        logger.info("="*60 + "\n")
        
        while self.running:
            try:
                # Listen for voice input
                text = self.voice_input.listen_for_command()
                
                if not text:
                    continue
                
                # Check for wake word
                wake_detected = self.wake_word.detect(text)
                
                if wake_detected:
                    # Play acknowledgment sound (optional)
                    await self.voice_output.speak("Yes?", play_audio=True)
                    
                    # If wake_detected is a string, it's the command after wake word
                    if isinstance(wake_detected, str) and wake_detected:
                        await self.process_voice_command(wake_detected)
                    else:
                        # Listen for actual command
                        logger.info("ðŸŽ¤ Listening for command...")
                        command = self.voice_input.listen_for_command()
                        
                        if command:
                            await self.process_voice_command(command)
                else:
                    logger.info("â­ï¸  No wake word detected, ignoring...")
                
                # Small delay before next listen
                await asyncio.sleep(0.5)
            
            except KeyboardInterrupt:
                break
            except Exception as e:
                logger.error(f"Error in voice loop: {e}")
                await asyncio.sleep(1)
    
    async def run(self):
        """Main run method"""
        
        await self.initialize()
        
        logger.info("\n" + "="*70)
        logger.info("ðŸŽ™ï¸  SARA VOICE ASSISTANT - WHATSAPP CONTROL")
        logger.info("="*70)
        logger.info("\nâœ… Features:")
        logger.info("  ðŸŽ¤ Voice commands via microphone")
        logger.info("  ðŸ“± WhatsApp control (send, read, reply)")
        logger.info("  ðŸ”Š Responses via computer speaker")
        logger.info("  ðŸ“¬ Background WhatsApp message monitoring")
        logger.info("\nðŸ“Š Status:")
        logger.info(f"  â€¢ WhatsApp: Connected âœ“")
        logger.info(f"  â€¢ Contacts: {len(self.contacts.contacts)} loaded")
        logger.info(f"  â€¢ Microphone: Ready")
        logger.info(f"  â€¢ Speakers: Ready")
        logger.info("\nðŸŽ¯ Commands You Can Say:")
        logger.info('  "Hey Sara, send a message to Pranav saying I\'ll be late"')
        logger.info('  "Sara, do I have any new messages?"')
        logger.info('  "Hey Sara, read messages from Mom"')
        logger.info('  "Sara, reply to Rajesh saying thanks"')
        logger.info("="*70 + "\n")
        
        # Start voice command loop
        self.running = True
        
        try:
            await self.voice_command_loop()
        except KeyboardInterrupt:
            logger.info("\nðŸ‘‹ Shutting down Sara...")
        finally:
            self.running = False
            await self.whatsapp.disconnect()


if __name__ == "__main__":
    assistant = SaraVoiceAssistant()
    asyncio.run(assistant.run())
```

---

## Part 3: Configuration

### Step 3.1: Update .env File

Add these settings to your existing `.env` file:

```env
# Existing Sara settings
GROQ_API_KEY=your_groq_api_key_here

# WhatsApp Bridge
WHATSAPP_BRIDGE_URL=http://localhost:3001

# Voice Settings
WHISPER_MODEL=base
TTS_VOICE=en-US-AriaNeural

# Paths
TEMP_DIR=temp
```

### Step 3.2: Update Requirements

Add these to your existing `requirements.txt`:

```txt
# WhatsApp Integration (add to existing requirements)
python-socketio[asyncio]==5.11.0
aiohttp==3.9.1
requests==2.31.0
```

> **Note:** Your existing requirements already have Whisper, TTS, and other dependencies.

---

## Part 4: Running the System

### Step 4.2: Install Python Dependencies

```bash
# Install new WhatsApp dependencies
pip install python-socketio[asyncio] aiohttp requests
```

> **Note:** Your existing Sara dependencies (Whisper, TTS, etc.) are already installed.

### Step 4.2: Start WhatsApp Bridge

```bash
# Terminal 1
cd whatsapp-bridge
npm run dev

# Wait for QR code, scan with WhatsApp
```

### Step 4.3: Start Voice Assistant

```bash
# Terminal 2 (from project root)
export GROQ_API_KEY=your_key_here
python src/main_voice_assistant.py
```

### Step 4.4: Use Voice Commands

```
Wait for: "ðŸŽ¤ LISTENING FOR WAKE WORD"

Then speak:
"Hey Sara, send a message to Pranav saying hello"

Sara will:
1. Detect wake word
2. Respond: "Yes?"
3. Process command
4. Execute WhatsApp action
5. Confirm: "Message sent to Pranav âœ“"
```

---

## ðŸŽ¯ Complete Usage Examples

### Example 1: Send Message

```
You (speaking): "Hey Sara, send a message to Pranav saying I'll be 10 minutes late"

Sara (speaker): "Yes?"
[processes]
Sara (speaker): "Message sent to Pranav Kumar âœ“"

[Pranav receives on WhatsApp]: "I'll be 10 minutes late"
```

### Example 2: Check Messages

```
You: "Sara, do I have any new messages?"

Sara: "Yes, you have 3 new messages. One from Mom saying 'When are you coming home?', one from Rajesh saying 'Thanks for yesterday', and one from Priya asking about the meeting."
```

### Example 3: Read and Reply

```
You: "Hey Sara, read messages from Mom"

Sara: "Mom sent: 'When are you coming home for dinner?'"

You: "Reply saying I'll be there by 8 PM"

Sara: "Replied to Mom âœ“"

[Mom receives on WhatsApp]: "I'll be there by 8 PM"
```

### Example 4: Complex Multi-Step

```
You: "Sara, check if I have messages from Rajesh and reply saying thanks"

Sara: "Rajesh sent: 'I've completed the report'. I've replied with 'thanks' âœ“"
```

---

## ðŸ”§ Two Modes of Operation

### Mode 1: Local Voice Commands (Your Microphone)

```
Your Computer Microphone
         â†“
    "Hey Sara, send message..."
         â†“
    Whisper transcribes
         â†“
    Groq processes
         â†“
    WhatsApp Bridge sends
         â†“
    Your Computer Speaker
    "Message sent âœ“"
```

### Mode 2: Incoming WhatsApp Messages (Background)

```
Someone sends WhatsApp message
         â†“
    Baileys receives
         â†“
    Python adds to queue
         â†“
    Sara announces via speaker
    "New message from Mom"
```

---

## ðŸŽ¨ Customization

### Change Wake Word

```python
# In src/main_voice_assistant.py
self.wake_word = WakeWordDetector(wake_words=["jarvis", "computer"])
```

### Change Voice

```python
# In .env file
TTS_VOICE=en-GB-SoniaNeural  # British female
TTS_VOICE=en-US-GuyNeural    # American male
```

### Adjust Whisper Model

```python
# In .env file
WHISPER_MODEL=tiny    # Fastest, less accurate
WHISPER_MODEL=base    # Balanced (recommended)
WHISPER_MODEL=small   # Better accuracy
WHISPER_MODEL=medium  # Best quality, slower
```

---

## ðŸ§ª Testing

### Test Microphone

```python
from src.connectors.voice_input import VoiceInput

voice = VoiceInput()
voice.list_microphones()
voice.test_microphone(duration=3)
```

### Test TTS

```python
from src.connectors.voice_output import VoiceOutput
import asyncio

voice_out = VoiceOutput()
asyncio.run(voice_out.speak("Hello, this is Sara!"))
```

### Test Wake Word

```python
from src.connectors.voice_input import WakeWordDetector

detector = WakeWordDetector(wake_words=["hey sara"])
print(detector.detect("hey sara send a message"))  # Returns: "send a message"
print(detector.detect("just talking"))  # Returns: False
```

---

## ðŸ”Š Audio Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         YOUR INTERACTION                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   You speak into     â”‚
        â”‚   Microphone         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   VAD detects        â”‚
        â”‚   speech start/end   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Whisper            â”‚
        â”‚   transcribes        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Wake word check    â”‚
        â”‚   "Hey Sara"         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Groq AI            â”‚
        â”‚   (function calling) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Execute WhatsApp   â”‚
        â”‚   CRUD operation     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Generate response  â”‚
        â”‚   with TTS           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Play through       â”‚
        â”‚   Computer Speaker   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š Performance Expectations

| Component | Latency | Notes |
|-----------|---------|-------|
| Wake word detection | ~100ms | Very fast |
| Voice recording | 1-3s | Depends on speech length |
| Whisper transcription | 200-500ms | "base" model |
| Groq processing | 200-300ms | With function calling |
| WhatsApp action | 100-200ms | Send message |
| TTS generation | 300-500ms | Edge-TTS |
| **Total** | **~2-5 seconds** | End-to-end |

---

## ðŸš€ Production Tips

### 1. Run as Background Service

```bash
# Create systemd service
sudo nano /etc/systemd/system/sara-voice.service
```

```ini
[Unit]
Description=Sara Voice Assistant
After=network.target

[Service]
Type=simple
User=youruser
WorkingDirectory=/path/to/sara-ai-project
Environment="GROQ_API_KEY=your_key"
ExecStart=/path/to/venv/bin/python src/main_voice_assistant.py
Restart=always

[Install]
WantedBy=multi-user.target
```

### 2. Auto-start on Boot

```bash
sudo systemctl enable sara-voice
sudo systemctl start sara-voice
```

### 3. Use Better Microphone

For better wake word detection:
- USB microphone array (e.g., ReSpeaker)
- Directional microphone
- Noise-canceling headset

---

## ðŸŽ¯ Summary

You now have a **local voice assistant** that:

âœ… Listens to your computer's microphone
âœ… Detects "Hey Sara" wake word
âœ… Transcribes your voice commands
âœ… Executes WhatsApp CRUD operations
âœ… Responds through computer speakers
âœ… Monitors incoming WhatsApp messages
âœ… Sends text messages to WhatsApp contacts (not voice notes)

**This integrates with your existing Sara AI project!** ðŸŽ‰

The voice assistant uses your existing Sara components and adds WhatsApp control capabilities through voice commands.


